import pandas as pd
import numpy as np
import matplotlib as plt

cropdf = pd.read_csv("Crop_recommendation.csv")
cropdf.head()

cropdf.shape

print('List of Columns',cropdf.columns)

print("Number of various crops: ", len(cropdf['label'].unique()))

print("List of crops: ", cropdf['label'].unique())

cropdf.info()

cropdf.hist()

cropdf.isnull().sum()

X = cropdf.drop('label', axis=1)
y = cropdf['label']

X

y

from sklearn.model_selection import train_test_split
X_train, X_test, Y_train, Y_test = train_test_split(X, y, test_size = 0.2,
shuffle = True, random_state = 0)

X_train

Y_train

def models(X_train, Y_train):
    # 1. K-Nearest Neighbors
    from sklearn.neighbors import KNeighborsClassifier
    knc = KNeighborsClassifier(n_neighbors=5, metric='minkowski', p=2)
    knc.fit(X_train, Y_train)

    # 2. Decision Tree
    from sklearn.tree import DecisionTreeClassifier
    dtc = DecisionTreeClassifier(random_state=0, criterion="entropy")
    dtc.fit(X_train, Y_train)

    # 3. Logistic Regression
    from sklearn.linear_model import LogisticRegression
    lr = LogisticRegression(random_state=0)
    lr.fit(X_train, Y_train)

    # 4. Random Forest
    from sklearn.ensemble import RandomForestClassifier
    rfc = RandomForestClassifier(random_state=0, criterion="entropy", n_estimators=10)
    rfc.fit(X_train, Y_train)

    # 5. Support Vector Machine
    from sklearn.svm import SVC
    svc = SVC(random_state=0)
    svc.fit(X_train, Y_train)

    # 6. Naive Bayes Classifier
    from sklearn.naive_bayes import GaussianNB
    nbc = GaussianNB()
    nbc.fit(X_train, Y_train)

    print('[0] K-Nearest Neighbors accuracy:', knc.score(X_train, Y_train))
    print('[1] Decision Tree accuracy:', dtc.score(X_train, Y_train))
    print('[2] Logistic Regression accuracy:', lr.score(X_train, Y_train))
    print('[3] Random Forest accuracy:', rfc.score(X_train, Y_train))
    print('[4] Support Vector Machine accuracy:', svc.score(X_train, Y_train))
    print('[5] Naive Bayes Classifier accuracy:', nbc.score(X_train, Y_train))

    return knc, dtc, lr, rfc, svc, nbc
model = models(X_train,Y_train)

from sklearn.ensemble import RandomForestClassifier
model1 = RandomForestClassifier()

model1.fit(X_train, Y_train)

y_pred = model1.predict(X_test)

def predict_new_data(N, P, K, T, H, PH, R):
    # Create a new DataFrame with the provided input features
    new_data = pd.DataFrame({
        'N': [N],
        'P': [P],
        'K': [K],
        'temperature': [T],
        'humidity': [H],
        'ph': [PH],
        'rainfall': [R]
    })
    # Predict using the model (assumes 'model1' is pre-trained and available)
    prediction = model1.predict(new_data)
    predicted_class = prediction[0]
    return predicted_class

N = str(input("Enter value for N (Nitrogen): "))
P = str(input("Enter value for P (Phosphorous): "))
K = str(input("Enter value for K (Potassium): "))
T = str(input("Enter value for temperature: "))
H = str(input("Enter value for humidity: "))
PH = str(input("Enter value for pH: "))
R = str(input("Enter value for rainfall: "))

# Call the prediction function
predicted_class = predict_new_data(N, P, K, T, H, PH, R)

# Print the prediction result
print(f"The predicted class for the input data is: {predicted_class}")
